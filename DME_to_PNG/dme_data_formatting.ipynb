{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm as tqdm\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import scipy.io as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../2015_BOE_Chiu\n"
     ]
    }
   ],
   "source": [
    "input_path = os.path.join('..', '2015_BOE_Chiu')\n",
    "print(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_path = [os.path.join(input_path, 'Subject_0{}.mat'.format(i)) for i in range(1, 10)] + [os.path.join(input_path, 'Subject_10.mat')]\n",
    "\n",
    "data_indexes = [10, 15, 20, 25, 28, 30, 32, 35, 40, 45, 50]\n",
    "\n",
    "width = 284\n",
    "height = 284\n",
    "width_out = 196\n",
    "height_out = 196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_nonzero(item):\n",
    "    if torch.count_nonzero(item)!=0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# function for resizing tensors to size, size\n",
    "def resize(item,size):\n",
    "    T = torchvision.transforms.Resize(size=(size,size), \n",
    "                                      interpolation=transforms.InterpolationMode.BILINEAR, \n",
    "                                      antialias=True)\n",
    "    return T(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCTDataset(Dataset):\n",
    "    def __init__(self, root, transforms, size):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.subject_path = [os.path.join(self.root, 'Subject_0{}.mat'.format(i)) for i in range(1, 10)]+[os.path.join(self.root, 'Subject_1{}.mat'.format(i)) for i in range(0,1)]\n",
    "        print(self.subject_path)\n",
    "        \n",
    "        self.size = size\n",
    "        self.images = torch.tensor([])\n",
    "        self.masks = torch.tensor([])\n",
    "        self.y = []\n",
    "        \n",
    "        self.load_images_and_masks()\n",
    "        \n",
    "        print(self.images.size(), self.masks.size())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "        \n",
    "        obj_ids = mask.unique()\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        num_objs = len(obj_ids)\n",
    "        \n",
    "        # one hot encoding masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "        boxes = []\n",
    "        \n",
    "        for i in range(num_objs):\n",
    "            \n",
    "            pos = torch.nonzero(masks[i])\n",
    "#             print(pos)\n",
    "            mins, _ = torch.min(pos, dim=0)\n",
    "            maxs, _ = torch.max(pos, dim=0)\n",
    "            \n",
    "#             print(pos)\n",
    "            xmin = mins[1]\n",
    "            ymin = mins[0]\n",
    "            \n",
    "            xmax = maxs[1]\n",
    "            ymax = maxs[0]\n",
    "            \n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            \n",
    "            \n",
    "        # mrcnn only needs boxes, labels, and masks for the target\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(obj_ids, dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        \n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        return img, target\n",
    "            \n",
    "        pass\n",
    "    \n",
    "    def load_images_and_masks(self):\n",
    "        \n",
    "        # for all images\n",
    "        for i in tqdm(range(len(self.subject_path))):  #range(len(self.subject_path))\n",
    "            \n",
    "            mat = io.loadmat(self.subject_path[i])\n",
    "            \n",
    "            images = np.expand_dims(np.transpose(mat['images'], (2,0,1))/255, 0)\n",
    "            y = np.transpose(mat['manualFluid1'], (2,0,1))\n",
    "            masks = np.expand_dims(np.nan_to_num(y),0)\n",
    "#             print(masks.shape)\n",
    "            \n",
    "            # convert to tensor\n",
    "            images = torch.as_tensor(images, dtype=torch.float32)\n",
    "            masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "            images = resize(images, self.size)\n",
    "            masks = resize(masks, self.size)\n",
    "            \n",
    "#             print(masks.size())\n",
    "\n",
    "            # we only keep images and masks with non-zero values\n",
    "            for idx in range(images.shape[1]):\n",
    "                \n",
    "                mask = masks[0][idx]\n",
    "                if check_for_nonzero(mask):\n",
    "                    temp1 = images[::, idx, ::]\n",
    "                    \n",
    "                    temp2 = masks[0, idx, ::].unsqueeze(0)\n",
    "                     \n",
    "                    # make image 3 channel instead of 1 -> (1, 3, H, W)\n",
    "                    img = torch.cat([temp1]*3).unsqueeze(0)\n",
    "#                     print(img.shape)\n",
    "                    \n",
    "                    self.images = torch.cat((self.images, img))\n",
    "                    self.masks = torch.cat((self.masks, temp2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/djp69/ECE661_mmSeg/DME_to_PNG/2015_BOE_Chiu/Subject_01.mat', '/home/djp69/ECE661_mmSeg/DME_to_PNG/2015_BOE_Chiu/Subject_02.mat', '/home/djp69/ECE661_mmSeg/DME_to_PNG/2015_BOE_Chiu/Subject_03.mat', '/home/djp69/ECE661_mmSeg/DME_to_PNG/2015_BOE_Chiu/Subject_04.mat', '/home/djp69/ECE661_mmSeg/DME_to_PNG/2015_BOE_Chiu/Subject_05.mat', '/home/djp69/ECE661_mmSeg/DME_to_PNG/2015_BOE_Chiu/Subject_06.mat', '/home/djp69/ECE661_mmSeg/DME_to_PNG/2015_BOE_Chiu/Subject_07.mat', '/home/djp69/ECE661_mmSeg/DME_to_PNG/2015_BOE_Chiu/Subject_08.mat', '/home/djp69/ECE661_mmSeg/DME_to_PNG/2015_BOE_Chiu/Subject_09.mat', '/home/djp69/ECE661_mmSeg/DME_to_PNG/2015_BOE_Chiu/Subject_10.mat']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:19<00:00,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([78, 3, 512, 512]) torch.Size([78, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "OCT = OCTDataset('/home/djp69/ECE661_mmSeg/DME_to_PNG/2015_BOE_Chiu', \n",
    "                 None, \n",
    "                 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to PNG Images and Folder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "def tensor_to_pil(tensor, mode='RGB'):\n",
    "    return ToPILImage(mode=mode)(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "indices = torch.randperm(len(OCT)).tolist()\n",
    "train_indices = indices[:-8]\n",
    "test_indices = indices[-8:]\n",
    "\n",
    "#change dir to corressponding dir\n",
    "train_dir = \"/home/djp69/ECE661_mmSeg/DME_to_PNG/STARE/images/train\"\n",
    "val_dir = \"/home/djp69/ECE661_mmSeg/DME_to_PNG/STARE/images/val\"\n",
    "\n",
    "target_train_dir = \"/home/djp69/ECE661_mmSeg/DME_to_PNG/STARE/annotation/train\"\n",
    "target_val_dir = \"/home/djp69/ECE661_mmSeg/DME_to_PNG/STARE/annotation/val\"\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(target_train_dir, exist_ok=True)\n",
    "os.makedirs(target_val_dir, exist_ok=True)\n",
    "\n",
    "for idx in train_indices:\n",
    "    img_tensor, _ = OCT[idx]  # Only need the image part here\n",
    "    mask_tensor = OCT.masks[idx]\n",
    "\n",
    "    # Convert tensors to PIL Images\n",
    "    pil_image = tensor_to_pil(img_tensor)\n",
    "    pil_mask = tensor_to_pil(mask_tensor, mode='L')  # Grayscale for mask\n",
    "\n",
    "    # Save the image and mask as PNG\n",
    "    pil_image.save(os.path.join(train_dir, f\"sample_{idx:02d}.png\"))\n",
    "    pil_mask.save(os.path.join(target_train_dir, f\"sample_{idx:02d}.ah.png\"))\n",
    "\n",
    "for idx in test_indices:\n",
    "    img_tensor, _ = OCT[idx]  # Only need the image part here\n",
    "    mask_tensor = OCT.masks[idx]\n",
    "\n",
    "    # Convert tensors to PIL Images\n",
    "    pil_image = tensor_to_pil(img_tensor)\n",
    "    pil_mask = tensor_to_pil(mask_tensor, mode='L')  # Grayscale for mask\n",
    "\n",
    "    # Save the image and mask as PNG\n",
    "    pil_image.save(os.path.join(val_dir, f\"sample_{idx:02d}.png\"))\n",
    "    pil_mask.save(os.path.join(target_val_dir, f\"sample_{idx:02d}.ah.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
